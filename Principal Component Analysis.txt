tinyurl.com/yyq9zmyo

########################################################################################################################

												PCA 

########################################################################################################################

** Scaling is mandatory for PCA **

1) Improving Signal to Noise Ratio (Improve the data quality) 
2) Feature Extraction => Creating derived features - Blackbox features => Not explainable 
3) Feature selection/Dimensionality Reduction 

Signal => Diagonals of pairplot [Distribution of features]
Noise  => High correlation between features [Non diagonals of pairplot => Relationship between features]

Multicolinearity => Relation between features 
Explainable AI 
PCA used in Radar, redio receiver to increase SNR 

1) Shift origin to the mean (F1` will be F1-3)
2)       F1  F2  F1`  F2`  PC1         PC2 
         
         1   1   -2   -2   -2sqrt(2)   0
         2   2   -1   -1   -1sqrt(2)   0
         3   3    0    0    0          0
         4   4    1    1   sqrt(2)     0
         5   5    2    2   2sqrt(2)    0
         
M        3   3    0    0      0        0
Variance          2    2      4        0

3) Number of PC's = Number of original features 
4) PC1 is regression line (PC1=PC2) => PC1 is best fit line 
5) PC2 will be 90 degrees to PC1 (All values will be 0 (as points will be on PC1 line)) => PC1 and PC2 will be new axes. 
6) Zero correlation between PC1 and PC2 => This means high SNR 
7) Variance (PC1,PC2) = Variance(F1`) + Variance(F2`) (4=2+2)
8) Always do scaling before PCA
9) PC2 is not required (variance is zero) 
10) Unit vector => Magnitude of vector is 1. w1x1+w2x2+b=0 => Magnitude is Sqrt(w1^2+w2^2) = ||w||

   F1`=F2` => F1`-F2`=0 => w1x1+w2x2=0 (Here w1=1,w2=1) ||w||=sqrt(2) != 1
   To convert to unit vector divide by ||w||
   F1`/sqrt(2) - F2`/sqrt(2) = 0 => Equation of PC1 

11) Eigen Vectors for PC1 => (1/sqrt(2),1/sqrt(2)) => Magnitude will be 1 => Eigen vectors is units of weights in unit magnitude 
12) Eigen Vectors for PC2 => (-1/sqrt(2),1/sqrt(2)) and Eigen value of PC2 is 0. 
13) Eigen Value is variance of PC1= 4 => SSE 
14) Slopes of perpendicular lines is m1*m2=-1 

   y-y`/x-x` = -1 => y-0/x-0 = -1 => x+y=0 => Equation of PC2 

15) PC1 is most important feature 

For points (x,y) => Eigen vectors are (x/||w||, y/||w||) 
Distance of a point (x1,y1) from line=ax+by+c=0 => D=ax1+by1+c/(sqrt(a^2+b^2)

Limitation - 
---------------
There has to be linear relation between features else it will not work.

Difference between fit and fit_transform 
------------------------------------------
fit => Learning u and sigma (learning unique columns in onehot encoder)
transform => applying scaling (Transforming features) 

### First do train_test split and then only apply scaling in supervised learning 

pca.fit => Calculating eigen values, eigen vectors and best fit line 
pca.transform => Using eigen vectors, calculating PC1, PC2,...coordinates 

Yaan Lecun => Biggest Deep Learning researcher (MNIST Dataset => Handwritten digit dataset) 
Visualising high dimension => You tube video => Converted 784 dimension to 3 for visualization 

PCA in computer vision 
-----------------------
Machine learning doest work with unstructured data 
PCA was used by computer vision
Pixels can be converted to numpy array of pixels 
Darker the pixel => Higher the number 
white => 0 Dark => 255

28*28 = 784 pixels (PCA changes to 40) 

Unstructured to Structured data conversion 
PCA is extremely useful for unstructured data as explaination not required (Computer vision, NLP, Image)

t-SNE => Clusters these images of numbers (cluster of 5,3,...) => t-SNE results are non deterministic but PCA are deterministic
mnist pca tsne 

SCREE PLOT => Used for cumulative plot of varaince 

EigenFace
------------
Eigenfaces => Principal Components (Hidden Feature) of image 
eigenface 0 tells background of the image, pimple on face shown in last PC 

https://scikit-learn.org/stable/auto_examples/applications/plot_face_recognition.html

All sklearn classification follows OBR(One vs rest classification)

PCA improves clustering => Since it reduces noise  

Customer Segmentation 
-----------------------
RFM => Recency (Days since last purchase), Frequency(Total number of purchases), Monetory(Total money this customer spent)
RFM Score = R_quartile+F_quartile+M_quartile => 444 => Champion customer, 144=> Lost customer (Used to visit frequently and used spend more), 111=> Worst customers

Before applying clustering => Ensures scaling, Remove multicolinearity, Remove skew.

########################################################################################################################

												Remove Skew

########################################################################################################################

Skews will create unnessary clustering i.e. skewed result of clustering

     log     Sqrt 
1     0       1
10    1       3.16
100   2       10
900   2.95    30

Using log transform, squareroot transform, power transform

Box-Cox 
---------
This decides idea transform to apply to remove skew. 

Mathematics behind PCA - 
--------------------------
Eigen Decomposition => Using covariance matrix decompose into eigen values and eigen vectors. 

COV(x,y) = A(Transpose)*A
A*v = Lambda*I*v 

A=> Covariance Matrix 
v=> Eigen Vector 
Lambda=> Eigen Value 

(A-Labda*I)*v=0
det(A-Labda*I) = 0 
From this we can calculate Lambda (Eigen Value) 
after this we will calculate eigen vectors v=[x1 x2]

Polynomialfeature improves correlation and we can apply PCA 
pipeline 
n_components in PCA is also hyperparameter 