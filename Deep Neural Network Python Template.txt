1) Install packages - 

   !pip install Keras
   !pip install tensorflow==2.0
   $ pip install --no-binary=h5py h5py => To save model (model.save(<file_name>)) => model=tf.keras.models.load_model(<file_name>)

2) Packages - 

   # Using Tensorflow Keras instead of the original Keras
   
   from tensorflow.keras.datasets import mnist
   from tensorflow.keras import Sequential
   from tensorflow.keras.layers import Dense,Dropout
   from tensorflow import keras
   from tensorflow.keras.layers import Flatten
   from tensorflow.keras.optimizers import SGD
   from tensorflow.keras.callbacks import EarlyStopping
   from tensorflow.keras.layers import BatchNormalization
   import tensorflow.keras
   from tensorflow.keras.constraints import max_norm
   from sklearn import metrics
   import tensorflow as tf
   from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping
   from tensorflow.keras.layers import Conv2D, MaxPooling2D
   from keras.wrappers.scikit_learn import KerasRegressor
   from keras.wrappers.scikit_learn import KerasClassifier 
   
3) Importing mnist data - 

   (xtrain,ytrain),(xtest,ytest)=mnist.load_data()
   (xtrain,ytrain),(xtest,ytest)=keras.datasets.fashion_mnist.load_data()
   
4) Reshape Features - 

   Xtrain=X_train.reshape(X_train.shape[0],X_train.shape[1],1)
   Xtest=X_test.reshape(X_test.shape[0],X_test.shape[1],1)

5) Convert labels to one-hot encoders - 

   y_train = keras.utils.to_categorical(ytrain, 10)
   y_test = keras.utils.to_categorical(ytest, 10)

6) Scaling - 

   # convert from integers to floats
   xtrain = xtrain.astype('float32')
   xtest = xtest.astype('float32')
   
   xtrain = xtrain / 255.0
   xtest = xtest / 255.0

7) Build Neural Network - 

   - The first layer in this network, tf.keras.layers.Flatten, transforms the format of the images from a two-dimensional array (of 28 by 28 pixels) to a one-dimensional array (of 
     28 * 28 = 784 pixels). Think of this layer as unstacking rows of pixels in the image and lining them up. This layer has no parameters to learn; it only reformats the data.
   - Each node contains a score that indicates the probability that the current image belongs to one of the 10 classes.
   
   model = Sequential()
   
   # Define model architecture
   
   model = Sequential()
   wt_init=tf.keras.initializers.he_normal(seed=None)
   
   # Define model architecture
   
   model.add(Flatten(input_shape=(28, 28)))
   model.add(Dense(128, activation ='relu',kernel_initializer = wt_init, kernel_constraint = max_norm(2)  ))
   model.add(BatchNormalization())
   model.add(Dropout(0.1)) 
   model.add(Dense(128, activation ='relu',kernel_initializer = wt_init, kernel_constraint = max_norm(2) ))
   model.add(Dropout(0.2)) 
   model.add(Dense(10,activation='softmax'))
   
   opt = tf.keras.optimizers.Adam(learning_rate=0.001,decay=1e-6)
   model.compile(loss='categorical_crossentropy',optimizer=opt,metrics=['accuracy'])
   
8) Model.fit - 
   
   #Adding Early stopping callback to the fit function is going to stop the training,
   #if the val_loss is not going to change even '0.001' for more than 10 continous epochs

   early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.001, patience=10)
   
   #Adding Model Checkpoint callback to the fit function is going to save the weights whenever val_loss achieves a new low value. 
   #Hence saving the best weights occurred during training

   model_checkpoint =  keras.callbacks.callbacks.ModelCheckpoint('mnist_cnn_checkpoint_{epoch:02d}_loss{val_loss:.4f}.h5',
                                                             monitor='val_loss',
                                                             verbose=1,
                                                             save_best_only=True,
                                                             save_weights_only=True,
                                                             mode='auto',
                                                             period=1)
   epochs = 10
   batch_size = 32 
   
   history = model.fit(xtrain, y_train, batch_size=batch_size, epochs=epochs, validation_split=.1,  
                       verbose=True,initial_epoch=0,callbacks=[tensorboard,early_stopping,model_checkpoint])  => Starts from 0 (initial_epoch) 
   
   loss,accuracy  = model.evaluate(xtest, y_test, verbose=False)

   #Testing the model on test set
   score = model.evaluate(x_test, y_test)
   print('Test loss:', score[0])
   print('Test accuracy:', score[1])
   
9) Model Prediction - 

   predictions = model.predict(xtest)
   predictions[0]
   np.argmax(predictions[0])
   ytest[0]

10) Classification Report - 

   y_pred = []
   for val in predictions:
       y_pred.append(np.argmax(val))
   
   cm = metrics.confusion_matrix(ytest,y_pred)
   print(cm)
   
   cr=metrics.classification_report(ytest,y_pred)
   print(cr)

11) Early stopping - 

   # simple early stopping
   es = EarlyStopping(monitor='val_loss', mode='min', verbose=1)
   
   # fit model
   history = model.fit(trainX, trainy, validation_data=(testX, testy), epochs=4000, verbose=0, callbacks=[es])

   OR 
   
   # patient early stopping
   es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=200)

   OR 
   
   es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=200)
   mc = ModelCheckpoint('best_model.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)
   # fit model
   history = model.fit(trainX, trainy, validation_data=(testX, testy), epochs=4000, verbose=0, callbacks=[es, mc])

   model.save('model_reader.model')
   new_model = tf.keras.models.load_model('model_reader.model')

12) Visualize mnist data 

   mnist-fashion_mnist data - 
   --------------------------
   
   class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',
                  'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']
   
   plt.figure(figsize=(10,10))
   for i in range(10):
       plt.subplot(1,10,i+1)
       plt.xticks([])
       plt.yticks([])
       plt.grid(False)
       plt.imshow(xtrain[i], cmap='gray')
       plt.xlabel(class_names[ytrain[i]])
   plt.show()

   mnist digits - 
   ---------------

   fig=plt.figure(figsize=(8, 8))
   columns = 10
   rows = 10
   for i in range(1, columns*rows +1):
       img = x_test[i]
       fig.add_subplot(rows, columns, i)
       plt.imshow(img, cmap='gray')
   plt.show()

13) Logging data for TensorBoard 

   Callbacks allows to call certain functions while model is being trained 

   tensorboard = tf.keras.callbacks.TensorBoard(log_dir='/tmp/mnist/dnn_v1',profile_batch = 100000000)  => Before model.fit()
   history = model.fit(xtrain, y_train, batch_size=batch_size, epochs=epochs, validation_split=.1, verbose=True,callbacks=[tensorboard])
   model.save('/tmp/mnist/v3.h5')  => After model.fit() 

14) Running TensorBoard - Run below command in Anaconda Prompt 

   tensorboard --logdir=/tmp/mnist/dnn_v1
   
15) Expand dimension 

   in_data=np.expand_dims(testX[0],axis=0)   #To feed 3-D data 
   
16) Mount Drive in Google Collab 

   Edit > Notebook Settings > Hardware Accelerator > Select GPU > save and connect 

   from google.colab import drive 
   drive.mount('/content/drive') 
   
   import h5py 
   h5f = h5py.File('/content/drive/My Drive/DLCP/....h5','r')    
   
17) GridSearchCV - 

# Function to create model, required for KerasClassifier
def create_model(number_of_layers=2, neurons_per_layer = 32, dropout_percent = 0.2, optimizer='adam'):
    # create model
    model = Sequential()
    model.add(Dense(neurons_per_layer, activation='relu', input_shape=(784,)))
    
    for i in range(number_of_layers-1):
        model.add(Dense(neurons_per_layer, activation='relu', kernel_constraint = maxnorm(3)))
        model.add(Dropout(dropout_percent))
        
    # Add the output layer with softmax
    model.add(Dense(num_classes, kernel_initializer = 'normal', activation='softmax'))  
        
    
    # Compile model
    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])
   
    return model
	
# create model
kc_model = KerasClassifier(build_fn=create_model,number_of_layers=2, neurons_per_layer = 256, optimizer = 'adam', epochs=20, batch_size=30000, verbose=0) # Wrapper for scikitlearn API, provides 
                                                                                    # facility to get scores 

kfold = StratifiedKFold(n_splits = 2, shuffle=True, random_state = 64)
results = cross_val_score(kc_model, x_train, original_y_train, cv=kfold, verbose=1)
print('results = {}\nresults.mean = {}'.format(results, results.mean()))
# define the grid search parameters

param_grid = dict(number_of_layers = [2 , 3],
                 neurons_per_layer = [32 , 128],
                 optimizer = ['adam', 'sgd'])

gridsearcher = GridSearchCV(estimator=kc_model, param_grid=param_grid, n_jobs=1)
grid_result = gridsearcher.fit(x_train, original_y_train)

print("Best: %f using %s" % (grid_result.best_score_, grid_result.best_params_))
means = grid_result.cv_results_['mean_test_score']
stds = grid_result.cv_results_['std_test_score']
params = grid_result.cv_results_['params']
for mean, stdev, param in zip(means, stds, params):
    print("%f (%f) with: %r" % (mean, stdev, param))

18) Keras Tuner - 

!pip install keras-tuner
from kerastuner import RandomSearch
from kerastuner.engine.hyperparameters import HyperParameters

def build_model(hp):  
  model = keras.Sequential([
    keras.layers.Conv2D(
        filters=hp.Int('conv_1_filter', min_value=32, max_value=128, step=16),
        kernel_size=hp.Choice('conv_1_kernel', values = [3,5]),
        activation='relu',
        input_shape=(28,28,1)
    ),
    keras.layers.Conv2D(
        filters=hp.Int('conv_2_filter', min_value=32, max_value=64, step=16),
        kernel_size=hp.Choice('conv_2_kernel', values = [3,5]),
        activation='relu'
    ),
    keras.layers.Flatten(),
    keras.layers.Dense(
        units=hp.Int('dense_1_units', min_value=32, max_value=128, step=16),
        activation='relu'
    ),
    keras.layers.Dense(10, activation='softmax')
  ])
  
  model.compile(optimizer=keras.optimizers.Adam(hp.Choice('learning_rate', values=[1e-2, 1e-3])),
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])
  
  return model
  
tuner_search=RandomSearch(build_model,
                          objective='val_accuracy',
                          max_trials=5,directory='output',project_name="Mnist Fashion")
						  
tuner_search.search(train_images,train_labels,epochs=3,validation_split=0.1)
tuner.get_best_hyperparameters()[0].values
tuner.get_best_models()[0].summary()
model=tuner_search.get_best_models(num_models=1)[0]
model.fit(train_images, train_labels, epochs=10, validation_split=0.1, initial_epoch=3)

19) Uploading Kaggle dataset to google collab -  

   from google.colab import drive
   drive.mount('/content/gdrive')

   # Colab library to upload files to notebook
   from google.colab import files
   
   # Install Kaggle library
   !pip install -q kaggle
   
   
   # Download kaggle.json file from Kaggle 
   uploaded = files.upload()
   
   !mkdir -p ~/.kaggle
   !cp kaggle.json ~/.kaggle
   # Change the permission
   !chmod 600 ~/.kaggle/kaggle.json
   
   !kaggle datasets download -d nih-chest-xrays/sample
   
   from zipfile import ZipFile
   file_name = 'sample.zip'
   
   with ZipFile(file_name,'r') as zip:
     zip.extractall()
     print('Done')

   !kaggle datasets list -s Chest
   
20) To Remove files from Google drive (First Mount Drive) 
   
   import os

   delete_filepath = '/content/gdrive/My Drive/Colab Notebooks/Uploading ChestXray dataset from Kaggle to Colab.ipynb'
   
   open(delete_filepath, 'w').close() 
   os.remove(delete_filepath) #delete the blank file from google drive will move the file to bin instead

######################################################################################################################################################################

																	Visualizations 

######################################################################################################################################################################

1) Plotting first 10 images in the triaining set and their labels from fashion_mnist.

class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',
               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']

plt.figure(figsize=(10,10))
for i in range(10):
    plt.subplot(1,10,i+1)
    plt.xticks([])
    plt.yticks([])
    plt.grid(False)
    plt.imshow(xtrain[i], cmap='gray')
    plt.xlabel(class_names[ytrain[i]])
plt.show()

2) Regression prediction - 

a = plt.axes(aspect='equal')
plt.scatter(y_test, y_predict)
plt.xlabel('True Values')
plt.ylabel('Predictions')
lims = [0, 50]
plt.xlim(lims)
plt.ylim(lims)
plt.plot(lims, lims)

3) Plot History with accuracy and loss in one - 

# plot history
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.plot(history.history['loss'])
plt.title('model accuracy')
plt.ylabel('accuracy / loss')
plt.xlabel('epoch')
plt.legend(['training', 'validation'], loc='best')
plt.show()

4) Learning curves - 

# plot diagnostic learning curves
for i in range(len(histories)):
		# plot loss
		pyplot.subplot(211)
		pyplot.title('Cross Entropy Loss')
		pyplot.plot(histories[i].history['loss'], color='blue', label='train')
		pyplot.plot(histories[i].history['val_loss'], color='orange', label='test')
		# plot accuracy
		pyplot.subplot(212)
		pyplot.title('Classification Accuracy')
		pyplot.plot(histories[i].history['accuracy'], color='blue', label='train')
		pyplot.plot(histories[i].history['val_accuracy'], color='orange', label='test')
pyplot.show()

5) Loss and Accuarcy plotted separately 

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')

plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('model Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')

6) 

print(history.history['val_accuracy'])
print(history.history['accuracy'])

ta = pd.DataFrame(history.history['accuracy'])
va = pd.DataFrame(history.history['val_accuracy'])

tva = pd.concat([ta,va] , axis=1)
tva.boxplot()

######################################################################################################################################################################

Epochs and Batch Size - 
-------------------------
Assume you have a dataset with 200 samples (rows of data) and you choose a batch size of 5 and 1,000 epochs.
This means that the dataset will be divided into 40 batches, each with five samples. The model weights will be updated after each batch of five samples.
This also means that one epoch will involve 40 batches or 40 updates to the model.
With 1,000 epochs, the model will be exposed to or pass through the whole dataset 1,000 times. That is a total of 40,000 batches during the entire training process.

Batch Size - 
--------------
Batch Gradient Descent.      ( Batch Size = Size of Training Set )
Stochastic Gradient Descent. ( Batch Size = 1 )
Mini-Batch Gradient Descent. ( 1 < Batch Size < Size of Training Set )

In the case of mini-batch gradient descent, popular batch sizes include 32, 64, and 128 samples. You may see these values used in models in the literature and in tutorials.

- MAE loss is less affected by outliers than MSE 

######################################################################################################################################################################
Does a column look like a skewed Gaussian, consider adjusting the skew with a Box-Cox transform.
Does a column look like an exponential distribution, consider a log transform.
Does a column look like it has some features, but they are being clobbered by something obvious, try squaring, or square-rooting.
Can you make a feature discrete or binned in some way to better emphasize some feature.

If the dataset is small (less than 2000 examples) use batch gradient descent
For larger datasets, typical mini-batch sizes are 64, 128, 256 or 512. Of course, you mini-batch size must fit your CPU/GPU memory

######################################################################################################################################################################

HyperParameter Tuning - 
-------------------------
#learn_rate = [0.001, 0.002]
#momentum = [0.0, 0.1, 0.3]
#optimizer = ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']
#init_mode = ['uniform', 'lecun_uniform', 'normal', 'glorot_normal', 'glorot_uniform', 'he_normal', 'he_uniform']
# activation = ['relu', 'tanh', 'sigmoid']
# dropout_rate = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]
#neurons = [10, 12, 13, 14, 15]

#####################################################################################################################################################################

Weight Initializers - 
-----------------------
Zeros: Initializer that generates tensors initialized to 0.
Ones: Initializer that generates tensors initialized to 1.
Constant: Initializer that generates tensors initialized to a constant value.
RandomNormal: Initializer that generates tensors with a normal distribution.
RandomUniform: Initializer that generates tensors with a uniform distribution.
TruncatedNormal: Initializer that generates a truncated normal distribution.
VarianceScaling: Initializer capable of adapting its scale to the shape of weights.
Orthogonal: Initializer that generates a random orthogonal matrix.
Identity: Initializer that generates the identity matrix.
lecun_uniform: LeCun uniform initializer.
glorot_normal: Glorot normal initializer, also called Xavier normal initializer.
glorot_uniform: Glorot uniform initializer, also called Xavier uniform initializer.
he_normal: He normal initializer.
lecun_normal: LeCun normal initializer.
he_uniform: He uniform variance scaling initializer.

#####################################################################################################################################################################

Useful Blogs - 

https://www.kdnuggets.com/2020/06/deep-learning-detecting-pneumonia-x-ray-images.html
https://towardsdatascience.com/announcing-pycaret-an-open-source-low-code-machine-learning-library-in-python-4a1f1aad8d46