Machine Learning Python Coding Template
----------------------------------------

############################################################################################################################################################   

													 Packages to be installed 

############################################################################################################################################################   

pip install shap
pip install impyute => For MICE 
pip install imblearn
!pip install pyforest

import pyforest

############################################################################################################################################################   

													  Machine Learning Packages 

############################################################################################################################################################   

1) Packages - 

   import numpy as np
   import pandas as pd
   
   # Used for displaying multiple o/ps without print 
   
   from IPython.core.interactiveshell import InteractiveShell
   InteractiveShell.ast_node_interactivity = "all"
   
   Stats Packages - 
   -----------------
   import scipy.stats as stats
   from scipy.stats import ttest_1samp,ttest_ind,mannwhitneyu,levene,shapiro,wilcoxon
   from scipy.stats import zscore
   from scipy.stats import ttest_rel
   from scipy.stats import bartlett
   from scipy.stats import f_oneway
   import statsmodels.api as sm
   from statsmodels.formula.api import ols
   from statsmodels.stats.multicomp import pairwise_tukeyhsd
   from statsmodels.stats.multicomp import MultiComparison
   from scipy.stats import chi2_contingency
   from statsmodels.stats.power import ttest_power
   from scipy.stats import norm 
   from scipy.stats import kstest
   from statsmodels.stats.diagnostic import lilliefors
   
   EDA packages - 
   ---------------
   from sklearn import model_selection
   from sklearn import preprocessing
   from sklearn.model_selection import train_test_split
   
   from scipy.stats import zscore
   from sklearn.decomposition import PCA
   from sklearn.preprocessing import PolynomialFeatures
   
   Up/Down Sampling packages - 
   ---------------------------- 
   Supervised Learning Algorithms Packages - 
   ------------------------------------------
   Ensemble packages - 
   -----------------------------
   Unsupervised Learning Algorithm packages - 
   --------------------------------------------
   Metrics packages - 
   -------------------
   
   Pipeline packages - 
   --------------------
   from sklearn.pipeline import Pipeline
   from sklearn.pipeline import make_pipeline
   
   Recommendation Systems - 
   -------------------------
   Hyperparameter packages - 
   -----------------------------------
   
   Plot packages - 
   ----------------
   import matplotlib.pyplot as plt
   import seaborn as sns
   import pylab

############################################################################################################################################################   

														   Data Preprocessing 

############################################################################################################################################################   

1) Reading file and operations on attributes

   DF = pd.read_csv('students-DF.csv', sep=";")
   DF = pd.read_csv('ratings_Electronics.csv',header=None,names=['userId','productID','Rating','Timestamp'])  #If no headers in csv file 
   DF = pd.read_excel('Online Retail.xlsx')
   DF = pd.read_csv('output_list.txt', sep=" ", header=None) => DF.columns = ["a", "b", "c", "etc."]
   DF.head()
   DF.describe().transpose()
   DF.shape 
   DF.info()
   DF.dtypes
   DF.columns 															     //* Displays column names 
   DF['col'] = DF['col'].astype('category') 								 //* Convert column datatype to float64/int32/category/ //* Always convert object to categoical variables.
   DF = DF.replace('?', np.nan)            									 //* Replace non numeric values(Ex-'?')in numerical columns with np.nan
   DF.replace(['basic.6y','basic.4y', 'basic.9y'], 'basic', inplace=True)    //* Replace 'basic.6y','basic.4y', 'basic.9y' with 'basic'
   DF['col'].value_counts()
   DF['column'].unique() OR DF.apply(lambda x: len(x.unique()))				 //* Cardinality 
   DF.corr()
   DF.rename(columns = {"C1": "NEW NAME","C2":"NEW NAME 2"}, inplace = True) //* Rename columns
   DF[DF['Experience'] < 0]['Experience'].count()                            //* Count of values having negative values in the column
   clusters_df = pd.DataFrame( { "num_clusters":cluster_range, "cluster_errors": cluster_errors} )	
   DF = DF[DF.Country.isin(['United Kingdom'])]
   df['Recency'] = (pd.datetime(2011,12,10) - pd.to_datetime(df['InvoiceDate'])).dt.days  //* Date to days 
   class_series=pd.Series(DF['class'])
   DF.drop(DF[DF['Experience']<0].index,axis=0,inplace=True)                 //* Dropping rows based on condition 
   DF.rename(columns={"A": "a", "B": "c"})                                   //* Column rename 
   DF.values                                                                 //* DF to Array 
   DF_copy = DF.copy(deep = True)
   df['Description'] = df['Description'].str.strip()                         //* Strip spaces 
   df.dropna(axis=0, subset=['InvoiceNo'], inplace=True)                     //* Drop row for column having null InvoiceNo
   df['title'].drop_duplicates(inplace=True)                                 //* Drop Duplicates 
   df.to_csv('filename.csv',index=False)                                     //* Writing dataframe to csv 
   for i in ["%.2d" % i for i in range(13)]:                                 //* 01 02...12
      print(i)
2) Check for non digit values in numerical column

   temp = pd.DataFrame(DF.column.str.isdigit())
   temp[temp['column'] == False]
   
3) Setting Index to column
   DF.index.name = 'number' //* Setting index name 
   DF.set_index('colname')  //* Setting index 
   
4) Splitting categorical/numerical data and copying to categorical/numerical dataframe
   
   cat_DF=DF.select_dtypes(exclude ='number')
   num_DF=DF.select_dtypes(include ='number')  
   
5) Dropping columns    //* Before dropping id column check if data is continuous by sorting 
   
   DF.drop(['traveltime', 'studytime', 'failures'],axis=1, inplace=True) //*Drop columns if DF.var()<1.00

6) Merging cat_data and num_data to common dataframe
   
   merge_data=pd.merge(num_data,cat_data,how='outer',on='number')  //* Here number is index common between both  
   
7) Copying input features to input dataframe

   X = merge_data.drop('Output Column Name',axis=1)   

8) Copying output feature to output dataframe
   
   Y = merge_data[['Output Column Name']]   
   
9) Log Transformation - 

   from scipy import stats
   col_tran=['DistanceFromHome','MonthlyIncome','PercentSalaryHike','YearsInCurrentRole','YearsWithCurrManager']
   for i in col_tran:
       df[i] = df[i].apply(lambda x:np.log(x+1))    

10) Distance between places 

   import geopy.distance as gd
   kal = (17.33,76.83)
   beng = (12.97,77.59)
   distance = gd.distance(kal,beng).km
   distance

11) OpenCV

   conda install -c conda-forge opencv   OR    pip install opencv-contrib-python 

   import cv2
   input = cv2.imread('2.jpg')
   cv2.imshow('Hello World',input)
   cv2.waitKey()
    
   print input.shape

12) Saving to CSV and submitting test results 
    
   ytest_pred1 = model1.predict(X_test)	
   DF_t = pd.read_csv('test.csv')
   
   submission1 = pd.DataFrame({'ID':DF_t['ID'],'Class':ytest_pred1})
   submission1.to_csv('Submission1.csv',index=False)

############################################################################################################################################################ 
					
															 Feature Engineering 

############################################################################################################################################################ 

1) Extract Title from name 

   df['Title'] = df.Name.apply(lambda x: re.search(' ([A-Z][a-z]+)\.', x).group(1))
   df['Title'] = df['Title'].replace({'Mlle':'Miss', 'Mme':'Mrs', 'Ms':'Miss'})
   df['Title'] = df['Title'].replace(['Don', 'Dona', 'Rev', 'Dr',
                                            'Major', 'Lady', 'Sir', 'Col', 'Capt', 'Countess', 'Jonkheer'],'Special')
   sns.countplot(x='Title', data=df);
   plt.xticks(rotation=45);
   
2) Replace Cabin column with Has_Cabin or not column 

   df['Has_Cabin'] = ~df.Cabin.isnull()   

3) Replace Parch, SibSp columns with Fam_Size column 

   df['Fam_Size'] = df.Parch + df.SibSp

############################################################################################################################################################   

														Jupyter Widgets and extensions 

############################################################################################################################################################   

https://towardsdatascience.com/bringing-the-best-out-of-jupyter-notebooks-for-data-science-f0871519ca29

pip install ipywidgets
jupyter nbextension enable --py widgetsnbextension

pip install qgrid
jupyter nbextension enable --py --sys-prefix qgrid

1) Qgrid  

   import qgrid
   qgrid_widget=qgrid.show_grid(df,show_toolbar=True)
   qgrid_widget

2) Snippets 

   git clone git://github.com/moble/jupyter_boilerplate
   jupyter nbextension install jupyter_boilerplate
   jupyter nbextension enable jupyter_boilerplate/main
   
############################################################################################################################################################   

														Handling Missing Values 

############################################################################################################################################################   

https://scikit-learn.org/stable/modules/impute.html#impute => Imputation Techniques 
pip install impyute => For MICE 
pip install missingpy

Packages - 
------------

from sklearn.preprocessing import Imputer
from sklearn.impute import SimpleImputer
from sklearn.impute import KNNImputer
from missingpy import KNNImputer
from impyute.imputation.cs import mice
from missingpy import MissForest
import missingno as msno 

1) mark zero values as missing or NaN => This is in cases like where cell radius cannot be zero 
   
   DF[[1,2,3,4,5]] = DF[[1,2,3,4,5]].replace(0, numpy.NaN)
   
2) Counting missing values    
   
   DF.isnull().sum()
   
   Plotting missing values - 
   
   msno.matrix(train)
   msno.bar(train) 
   
3) Dropping missing value columns - 

   DF.dropna(inplace=True)
  
4) Fillna - 

   Replacing null values with mean/median/mode - 
   DF = DF.apply(lambda x: x.fillna(x.median()),axis=0)
 
   OR 
  
   DF.fillna(DF.median(),inplace=True)
   
   OR 
   
   data['Embarked'] = data['Embarked'].fillna('S')
   
5) Imputer - 
   
   imputer=Imputer(missing_values='NaN', strategy='median')         //* If outliers always safe to use median 
   trans_DF=imputer.fit_transform(DF_array)   						//* X is input array 
   
   # If we replace missing values with median we are sharpening/reducing variance of normal distribution => Biasing the dataset
   # If missing column is highly correlated with another column then use that column to calculate missing values.(MICE => Multivariate Imputation via Chained Equations)
 
6) KNNImputer - 
 
   imputer = KNNImputer(n_neighbors=2, weights="uniform")
   df[['col']]= imputer.fit_transform(df[['col']])
 
7) MICE 
   
   X = data.drop('Survived', axis=1)

   imputed = mice(X.values)
   mice_ages = imputed[:, 2]

8) MissForest 

   imputer = MissForest()
   df[['col']]= imputer.fit_transform(df[['col']])
   
9) Handling negative values - 

   neg_col=['Overtime','Other Salaries','Other Salaries','Retirement','Health/Dental','Other Benefits','Total Benefits']
   for i in neg_col:
       print(df[df[i] < 0].shape)
   
   for j in neg_col:
    ind=df[(df[j]<0) & (df[j] == 1)].index
    df.drop(ind,axis=0,inplace=True)
   
############################################################################################################################################################   

														      Handling Outliers 

############################################################################################################################################################   

   Finding outliers - Boxplot,scatter plot, Z-score, IQR
   
1) IQR method 
   
   Q1 = DF["MDVP:Fhi(Hz)"].quantile(0.25)
   Q3 = DF["MDVP:Fhi(Hz)"].quantile(0.75)
   IQR = Q3 - Q1
   Outliers = (DF["MDVP:Fhi(Hz)"] < (Q1 - 1.5 * IQR)) |(DF["MDVP:Fhi(Hz)"] > (Q3 + 1.5 * IQR))
   Outliers[Outliers==True]
   
   OR 
   
   # calculate interquartile range
   q25, q75 = percentile(data, 25), percentile(data, 75)
   iqr = q75 - q25
   print('Percentiles: 25th=%.3f, 75th=%.3f, IQR=%.3f' % (q25, q75, iqr))
   
   # calculate the outlier cutoff
   cut_off = iqr * 1.5
   lower, upper = q25 - cut_off, q75 + cut_off
   
   # identify outliers
   outliers = [x for x in data if x < lower or x > upper]
   print('Identified outliers: %d' % len(outliers))
   
   # remove outliers
   outliers_removed = [x for x in data if x >= lower and x <= upper]
   print('Non-outlier observations: %d' % len(outliers_removed))
   
2) Z-score 
   If the z score is greater than 3 than we can classify that point as an outlier.
   
   outliers=[]
   def detect_outlier(data_1):
    
       threshold=3
       mean_1 = np.mean(data_1)
       std_1 =np.std(data_1)
       
       
       for y in data_1:
           z_score= (y - mean_1)/std_1 
           if np.abs(z_score) > threshold:
               outliers.append(y)
       return outliers
   outlier_datapoints = detect_outlier(dataset)
   print(outlier_datapoints)
   
   # Outliers can happen if we mix up guassian (Ex- in auo-mpg dataset we mix up small,medium,large cars) => Use clustering 
   # Long tails in kde => Outliers (If we replace outliers with median, we might generate new outliers and also normal distribution can become narrow) 
   # Capping - Outliers very close to 2*SD => Replace that data with 2*SD 
   
3) Greater than 2*SD 
   
    DF_X = DF.drop('status',axis=1)
    def replace(col):
        median, std = col.median(), col.std()  #Get the median and the standard deviation of every group 
        outliers = (col - median).abs() > 2*std # Subtract median from every member of each group. Take absolute values > 2std
        col[outliers] = col.median()       
        return col
    
    for i in range(0,12):
        no_outlier=DF_X.iloc[:,i].transform(replace)
    	
   ### Note: When we remove outliers and replace with median or mean, the distribution shape changes, the standard deviation becomes tighter creating new outliers. The new outliers would be much closer to the centre than original outliers so we accept them without modifying them.
      
4) Standard Deviation - 

   # calculate summary statistics
   data_mean, data_std = mean(data), std(data)
   
   # identify outliers
   cut_off = data_std * 3
   lower, upper = data_mean - cut_off, data_mean + cut_off
   
   # identify outliers
   outliers = [x for x in data if x < lower or x > upper]
   print('Identified outliers: %d' % len(outliers))
   
   # remove outliers
   outliers_removed = [x for x in data if x >= lower and x <= upper]
   print('Non-outlier observations: %d' % len(outliers_removed))
	  
############################################################################################################################################################   

														   Encoding Methods 

############################################################################################################################################################

1) Nominal Encoding 
   - One Hot Encoding 
   - One Hot Encoding with Many categories (KDD Orange Kaggle used) => https://www.kaggle.com/learn-forum/114857 
   - Mean Encoding 

2) Ordinal Encoding 
   - Label Encoding 
   - Target guided Ordinal Encoding 

Packages - 
-----------
from sklearn.preprocessing import LabelEncoder, OneHotEncoder

1)  Applying label encoder to categorical data 
   
   labelencoder = LabelEncoder()
   for i in range(17):
	   cat_data.iloc[:,i] = labelencoder.fit_transform(cat_data.iloc[:,i])
   OR
   
   DF['C1'] = labelencoder.fit_transform(DF['C1']) //*Only on one column 
   
   OR 
   
   labelencoder=LabelEncoder()
   for column in df.columns:
       df[column] = labelencoder.fit_transform(df[column])

   Inverse Transform of Label Encoder - 

   list(le.inverse_transform([2, 2, 1]))
   ['tokyo', 'tokyo', 'paris']	   

2) One hot encoder to apply on categorical data (https://www.kaggle.com/shahules/an-overview-of-encoding-techniques)
   
   enc = preprocessing.OneHotEncoder(categories=[genders, locations, browsers])
   X = onehotencoder.fit_transform(X).toarray()
   
3) Get Dummies 
   
   DF = pd.get_dummies(DF, columns=['origin'],drop_first=True)

4) Binning numerical columns

   interval = (18, 25, 35, 60, 120)
   age_cats = ['Student', 'Young', 'Adult', 'Senior']
   DF["Age_cat"] = pd.cut(DF.age, interval, labels=age_cats)

   OR
   
   data['CatAge'] = pd.qcut(data.Age, q=4, labels=False )
   data['CatFare']= pd.qcut(data.Fare, q=4, labels=False)
   data.head()

5) One Hot Encoding - variables with many categories

   https://www.kaggle.com/learn-forum/114857
   
   - df.apply(lambda x: len(x.unique()))
   - Top 10 most frequent categories for the variable 
     
	df['col'].value_counts().sort_values(ascending=False).head()
	 
   - List with the most frequent categories of the variable 
   
    top_10 = [x for x in df['col'].value_counts().sort_values(ascending=False).head(10).index]
   
   - get whole set of dummy variables, for all the categorical variables

    def one_hot_top_x(df,variable, top_x_labels):
        for label in top_x_labels:
            df[variable+'_'+label] = np.where(df[variable]==label, 1, 0)
    
    one_hot_top_x(df,'col',top_10) 	
    df.head()
	
6) Convert numerical column (Like 0-35% marks column as fail(0) else pass(1)) to categorical based on condition 
   
   for j in range(7,10):						//* 7 to 10 columns are converted here
       for i in range(st_data.iloc[0:,j].size): //* j = column number and i=cell position 
           if st_data.iloc[0:,j][i] < 35:
               st_data.iloc[0:,j][i] = 0
           else:
               st_data.iloc[0:,j][i] = 1
			   
   OR

   # Replace the numbers in categorical variables with the actual country names in the origin col
   DF['origin'] = DF['origin'].replace({1: 'america', 2: 'europe', 3: 'asia'})   
   
7) FeatureHasher
   
   X_train_hash=X.copy()
   for c in X.columns:
       X_train_hash[c]=X[c].astype('str')      
   hashing=FeatureHasher(input_type='string')
   train=hashing.transform(X_train_hash.values)
   
8) Encoding categories with dataset statistics (numeric representation for every category with a small number of columns but with an encoding that will put similar categories close to each other).

   X_train_stat=X.copy()
   for c in X_train_stat.columns:
       if(X_train_stat[c].dtype=='object'):
           X_train_stat[c]=X_train_stat[c].astype('category')
           counts=X_train_stat[c].value_counts()
           counts=counts.sort_index()
           counts=counts.fillna(0)
           counts += np.random.rand(len(counts))/1000
           X_train_stat[c].cat.categories=counts
		   
9) Encoding cyclic features 
   
   X_train_cyclic=X.copy()
   columns=['day','month']
   for col in columns:
       X_train_cyclic[col+'_sin']=np.sin((2*np.pi*X_train_cyclic[col])/max(X_train_cyclic[col]))
       X_train_cyclic[col+'_cos']=np.cos((2*np.pi*X_train_cyclic[col])/max(X_train_cyclic[col]))
   X_train_cyclic=X_train_cyclic.drop(columns,axis=1)
   
   X_train_cyclic[['day_sin','day_cos']].head(3)

   - Target encoding
   - K-Fold target encoding
   
############################################################################################################################################################

																Train Test Splitting

############################################################################################################################################################

from sklearn.preprocessing import LabelEncoder   

1) Training data and test data split 
   
   from sklearn.model_selection import train_test_split
   train , test = train_test_split(DF, test_size = 0.3,random_state=5)  //* 70:30 ratio
   
   X_train = train.drop('Personal Loan',axis=1)   
   X_test  = test.drop('Personal Loan',axis=1)   
   Y_train = train['Personal Loan']
   Y_test  = test['Personal Loan']

   OR 
   
   X = mpg_df.drop('mpg', axis=1)
   y = mpg_df[['mpg']]               =>Always use [[]] for target column else will get invalid index on scalar error on coef_ 
   X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=1,stratify=y)
   
   
############################################################################################################################################################

																 Scaling Methods 

############################################################################################################################################################

Packages - 
-----------
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import MinMaxScaler   
   
1) Scaling - Standard scalar(or Z score)/MinMaxScaler => Always apply on train_ data 
   
   sc=StandardScaler()
   sc.fit(X_train)              => Get mean,SD i.e. learning u and sigma 
   Xs_train = sc.transform(X_train)   => apply (x-u)/sigma 
   Xs_test = sc.transform(X_test)
   X_test = pd.DataFrame(Xs_test)
   
   OR 
   
   DF_scaled = DF.apply(zscore)
   
   OR
   
   from sklearn import preprocessing

   # scale all the columns of the mpg_df. This will produce a numpy array
   DF_scaled = preprocessing.scale(DF) //* Numpy array 
   DF_scaled = pd.DataFrame(DF_scaled, columns=DF.columns)
   
   OR 
   
   scaler = MinMaxScaler(feature_range=(0, 1))
   
   x_train_scaled = scaler.fit_transform(x_train)
   x_train = pd.DataFrame(x_train_scaled)
   
   x_test_scaled = scaler.fit_transform(x_test)
   x_test = pd.DataFrame(x_test_scaled)

   Linear model is impacted whether we Scale it or not, but ridge is impacted. 
############################################################################################################################################################	

												Handling Imbalanced dataset using imblearn package 

############################################################################################################################################################	

https://imbalanced-learn.readthedocs.io/en/stable/over_sampling.html => Sampling Techniques 

from imblearn.over_sampling import SMOTE
from imblearn.over_sampling import RandomOverSampler
from imblearn.under_sampling import NearMiss
from imblearn.under_sampling import RandomUnderSampler
from imblearn.under_sampling import TomekLinks
from imblearn.combine import SMOTETomek
from imblearn.combine import SMOTEENN
   
Over-Sampling 

###############

1)  SMOTE(Synthetic Minority Over Sampling Technique) -
    --------------------------------------------------------
    # Better recall in SMOTE than NearMiss
	
    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 1, stratify=y)
    smt = SMOTE()
    X_train, y_train = smt.fit_sample(X_train, y_train)
    np.bincount(y_train) 
	
	OR 
	
	sm = SMOTE(sampling_strategy = 1 ,k_neighbors = 5, random_state=1)   # SMOTE runs on minority class (Uses KNN algorithm on minority class alone)
    X_train_res, y_train_res = sm.fit_sample(X_train, y_train.ravel())   # ravel() creates data into 1-D array

2) KmeansSMOTE 
   ------------

3) SVMSMOTE 
   ----------
   
4) ADASYN
   --------
   
5) BorderlineSMOTE
   ----------------

6) SMOTENC 
   --------

   from imblearn.over_sampling import SMOTENC
   smote_nc = SMOTENC(categorical_features=[0, 2], random_state=0)
   X_resampled, y_resampled = smote_nc.fit_resample(X, y)

7) RandomOverSampler -
   ----------------------
   RandomOverSampler is over-sampling by duplicating some of the original samples of the minority class.
	
   ros = RandomOverSampler()
   X_ros, y_ros = ros.fit_sample(X_train, y_train)	

Under Sampling - 

##################

1) Tomek Links - 
   --------------
   tl = TomekLinks(return_indices=True, ratio='majority')
   X_tl, y_tl, id_tl = tl.fit_sample(X_train, y_train)   # id_tl is removed instances of majority class
	
2) NearMiss - 
   -----------
   X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 1, stratify=y)
   nr = NearMiss()
   X_train, y_train = nr.fit_sample(X_train, y_train)
   np.bincount(y_train)
	
3) RandomUnderSampler - 
   ----------------------
   rus = RandomUnderSampler(return_indices=True)
   X_rus, y_rus, id_rus = rus.fit_sample(X_train, y_train)   # id_rus is records left out after under_sampling
	
Combination of over-sampling and down-sampling - 

####################################################
	
1) SMOTETomek - 
   -------------
	SMOTE method can generate noisy samples by interpolating new points between marginal outliers and inliers. This issue can be solved by cleaning the space resulting from over-sampling. SMOTEENN tends to clean more noisy samples than SMOTETomek.
	
    smote_tomek = SMOTETomek(random_state=0)
    X_resampled, y_resampled = smote_tomek.fit_resample(X, y)
	
2) SMOTEEN - 
   ----------
	
	from imblearn.combine import SMOTEENN
    smote_enn = SMOTEENN(random_state=0)
    X_resampled, y_resampled = smote_enn.fit_resample(X, y)

############################################################################################################################################################	

											     			Feature Selection Techniques

############################################################################################################################################################	

1) PCA 
    
   sc = StandardScaler()
   X_std =  sc.fit_transform(X)          
   cov_matrix = np.cov(X_std.T)
   
   eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)
   
   # Make a set of (eigenvalue, eigenvector) pairs
   eig_pairs = [(eigenvalues[index], eigenvectors[:,index]) for index in range(len(eigenvalues))]
   
   # Sort the (eigenvalue, eigenvector) pairs from highest to lowest with respect to eigenvalue
   eig_pairs.sort()
   
   eig_pairs.reverse()
   print(eig_pairs)
   
   # Extract the descending ordered eigenvalues and eigenvectors
   eigvalues_sorted = [eig_pairs[index][0] for index in range(len(eigenvalues))]
   eigvectors_sorted = [eig_pairs[index][1] for index in range(len(eigenvalues))]

   tot = sum(eigenvalues)
   var_explained = [(i / tot) for i in sorted(eigenvalues, reverse=True)]  # an array of variance explained by each 
   # eigen vector... there will be 8 entries as there are 8 eigen vectors)
   cum_var_exp = np.cumsum(var_explained)  # an array of cumulative variance. There will be 8 entries with 8 th entry 
   # cumulative reaching almost 100%

   # P_reduce represents reduced mathematical space....
   P_reduce = np.array(eigvectors_sorted[0:7])   # Reducing from 8 to 4 dimension space
   X_std_4D = np.dot(X_std,P_reduce.T)   # projecting original data into principal component dimensions
   Proj_data_df = pd.DataFrame(X_std_4D)  # converting array to dataframe for pairplot
   
   OR 
   
   pca = PCA(n_components=2)
   pca.fit(X_scaled)
   X_pca = pca.transform(X_scaled)
   
   pca.components_
   #pca.explained_variance_
   pca.explained_variance_ratio_
   pca.explained_variance_ratio_.cumsum()
   
   model = DecisionTreeClassifier(max_depth=2, random_state=42)
   model.fit(pca.transform(X_train), y_train)
   
   model.score(pca.transform(X_test), y_test)
	
2) 	
	
############################################################################################################################################################

															Supervised Algorithms

############################################################################################################################################################

Packages - 
------------
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LinearRegression
from sklearn.linear_model import Ridge
from sklearn.linear_model import Lasso 
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn import svm
from sklearn.tree import DecisionTreeClassifier

1) KNN Model - 
   ------------
   Finding optimal value of K in KNN model 
   
   ac_scores = []
   
   for K in range(20):
       K = K+1
       Model = KNeighborsClassifier(metric= 'euclidean',n_neighbors=k)
       Model.fit(X_train, Y_train)
       Y_pred = Model.predict(Xs_test)
       score = accuracy_score(Y_test, Y_pred)
       ac_scores.append((score,K))
   
   For which value of K accuracy is high select that K value 

   Model = KNeighborsClassifier(metric= 'euclidean',n_neighbors=k)
   Model.fit(X_train, y_train)
   y_pred = NNH.predict(X_test)
   accuracy_score(y_test, y_pred))
   
2) Linear Regression - 
   ---------------------
   regression_model = LinearRegression()
   regression_model.fit(X_train, y_train)
   
   Coefficients of all input features  - 
   for idx, col_name in enumerate(X_train.columns):
       print("The coefficient for {} is {}".format(col_name, regression_model.coef_[0][idx]))
	   
   Intercept for the model - 
   intercept = regression_model.intercept_[0]

   y_pred = regression_model.predict(X_test)
   
3) Poynomial models - 
   -------------------
   Use it, If there is non-linear interaction between columns.
   Interaction_only=True => Take only those dimentions which shows correlation between themselves. 
   
   poly = PolynomialFeatures(degree=2, interaction_only=True)
   X_poly = poly.fit_transform(X_scaled)
   after applying regression_model, print(regression_model.coef_[0])
   
4) Ridge and Lasso model - 
   ------------------------
   Common models for regularization 
   The higher the alpha, the most feature coefficients are zero.
   When alpha is 0, Lasso regression produces the same coefficients as a linear regression. When alpha is very very large, all coefficients are zero.
   
   ridge=Ridge(alpha=0.3)
   ridge.fit(X_train,y_train)
   print("Ridge model : ",(ridge.coef_))
   
   lasso=Lasso(alpha=0.3)
   lasso.fit(X_train,y_train)
   print("Lasso model : ",(lasso.coef_))
   
   # Gives 0 coefficients i.e. drops the column from analysis 
   
5) Logistic Regression - 
   ----------------------
   model=LogisticRegression()
   model.fit(X_train, y_train)
   prediction=model.predict(X_test)
   model.coef_
   model.intercept_
   
6) NaiveBayes Model - 
   ----------------------
   from sklearn.naive_bayes import GaussianNB
   NB = GaussianNB()
   NB = NB.fit(Xis_train, yi_train)
   y_pred_NB=NB.predict(Xis_test)
   
7) Decision Tree 
   -------------- 
    # Decision tree in Python can take only numerical / categorical colums. It cannot take string / object types. 	

	for feature in credit_df.columns: 													# Loop through all columns in the dataframe
    if credit_df[feature].dtype == 'object': 											# Only apply for columns with categorical strings
        credit_df[feature] = pd.Categorical(credit_df[feature]).codes 				    # Replace strings with an integer
		
    dt_model=DecisionTreeClassifier(criterion='entropy',random_state=0)                 # Fully grown tree (high bias error and var error)
    dt_model2 = DecisionTreeClassifier(criterion = 'entropy', class_weight={0:.5,1:.5}, max_depth = 5, min_samples_leaf=5 )    # Regularized tree 
    
    dt_model.fit(X_train, y_train)
	print('Training accuracy',dt_model.score(X_train , y_train))
    print('Test accuracy',dt_model.score(X_test , y_test))

	print (pd.DataFrame(dt_model.feature_importances_, columns = ["Imp"], index = X_train.columns))

8) Support Vector Machine 
   ------------------------
    svc_model = SVC(C= .1, kernel='linear', gamma= 1)    //*kernel='rbf' or kernel='poly' or kernel='sigmoid'
    svc_model.fit(X_train, y_train)
    
    prediction = svc_model .predict(X_test)
    #pd.crosstab(y_test, y_pred)
	print(svc_model.score(X_train, y_train))
	print(svc_model.score(X_test, y_test))
	print("Confusion Matrix:\n",confusion_matrix(prediction,y_test))

############################################################################################################################################################

																Ensemble Techniques

############################################################################################################################################################

pip install xgboost 

Packages - 
------------
from sklearn.ensemble import BaggingClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.ensemble import RandomForestClassifier   
from sklearn.ensemble import ExtraTreesClassifier   
from xgboost import XGBClassifier

1) Ensemble Learning - Bagging 

    bgcl = BaggingClassifier(n_estimators=100, max_samples=.50 , oob_score=True)
    bgcl = bgcl.fit(credit_df, credit_labels)
    print(bgcl.oob_score_)

2) Ensemble Learning - Ada Boosting (For classification) 
   
   GridSearchCV helps in deciding n_estimators. Bagging or boosting if we dont give base_estimator, by default it uses decision tree algorithm. 
   All the Test records will go through all 50 trees and we predict based on majority voter. 
   If we dont use base_estimator, score will drop. 
   Everytime we run this score will change slighly since internally decision tree uses random_state. 
   This change in score can be avoided by setting random_state. 
   If we change n_estimators, score changes. We can use GridSearchCV. 
   
    abcl = AdaBoostClassifier(base_estimator=dt_model, n_estimators=50)
    abcl = abcl.fit(train_set, train_labels)
    
	test_pred = abcl.predict(test_set)
    abcl.score(test_set , test_labels)

3) Ensemble Learning - Gradient Boost

    gbcl = GradientBoostingClassifier(n_estimators = 50, learning_rate = 0.09, max_depth=5)
    gbcl = gbcl.fit(train_set, train_labels)
    test_pred = gbcl.predict(test_set)
    gbcl.score(test_set , test_labels)

4) Ensemble RandomForest Classifier - Designed specifically for decision trees. 
   By default it will assign n_estimators as 10. 

    rfcl = RandomForestClassifier(n_estimators = 6)
    rfcl = rfcl.fit(train_set, train_labels)
    test_pred = rfcl.predict(test_set)
    rfcl.score(test_set , test_labels)

5) Voting Classifier (Part of stacking)

   rf = RandomForestClassifier(n_estimators=51,criterion='entropy',random_state=0)
   gb = GradientBoostingClassifier(n_estimators=100,random_state=0)
   vc = VotingClassifier(estimators = [('rf',rf),('gb', gb)],voting='soft')

6) Stacking Classifier (Part of stacking)

   rf = RandomForestClassifier(n_estimators=51,criterion='entropy',random_state=0)
   gb = GradientBoostingClassifier(n_estimators=100,random_state=0)
   gbstack = GradientBoostingClassifier(n_estimators=200,random_state=0)
   sc = StackingClassifier(classifiers=[rf, gb], meta_classifier=gbstack)

7) XGBClassifier - 

   model = XGBClassifier()
   model.fit(X_train,y_train)
   
8) ExtraTreesClassifier - 

   model = ExtraTreesClassifier()
   model.fit(X_train,y_train)

############################################################################################################################################################

															Unsupervised Learning Algorithms 

############################################################################################################################################################

Packages - 
----------
from sklearn.cluster import KMeans
from sklearn.cluster import AgglomerativeClustering 
from sklearn.cluster import DBSCAN
   
1) KMeans Clustering

   Finding optimal number of clusters-
   ------------------------------------
   cluster_range = range( 1, 10)   # expect 3 to four clusters from the pair panel visual inspection hence restricting from 2 to 6
   cluster_errors = []
   for num_clusters in cluster_range:
        clusters = KMeans( num_clusters, n_init = 5)
        clusters.fit(X)
        labels = clusters.labels_                     # capture the cluster lables  labels_ or .predict (it is label 0, 1, 2, 3 cluster#) 
        centroids = clusters.cluster_centers_         # capture the centroids
        cluster_errors.append( clusters.inertia_ )    # capture the intertia
   
   clusters_df = pd.DataFrame( { "num_clusters":cluster_range, "cluster_errors": cluster_errors } )

   kmeans = KMeans(n_clusters=3, n_init = 15, random_state=2)
   kmeans.fit(DF_scaled)
   
   Finding centroids - 
   --------------------
   centroids=kmeans.cluster_centers_
   centroid_df = pd.DataFrame(centroids, columns = list(DF_scaled) )
   
   Applying Labels - 
   -------------------
   prediction=kmeans.predict(DF_scaled)
   DF_scaled["Label"] = prediction
   
   Splitting dataframe based on labels - 
   --------------------------------------
   # Split Dataframe based on labels 
   DF_small   = DF.drop(DF[(DF['Label']==1) | (DF['Label']==2)].index,axis=0)
   DF_medium  = DF.drop(DF[(DF['Label']==0) | (DF['Label']==2)].index,axis=0)
   DF_large   = DF.drop(DF[(DF['Label']==0) | (DF['Label']==1)].index,axis=0)
   
2) Hierarchical Clustering 

   model2 = AgglomerativeClustering(n_clusters=3, affinity='euclidean',  linkage='ward')
   model2.fit(df_scaled)
   L=pd.DataFrame(model2.labels_)
   L[0].value_counts()

3) DBSCAN 

   #Initialize DBSCAN
   db = DBSCAN(eps=0.2, min_samples=10)
   #Fit on the data
   db.fit(df[['Xcircle_X1', 'Xcircle_X2']])
   np.unique(db.labels_)
   df['DB_Cluster'] = db.labels_
   
   sns.scatterplot(x='Xcircle_X1', y='Xcircle_X2', hue='DB_Cluster', data=df, palette='spring')
   
############################################################################################################################################################	

															Accuracy, error calculation Metrics

############################################################################################################################################################	

Packages - 
-----------
from sklearn import metrics
from sklearn.metrics import classification_report
from sklearn.metrics import accuracy_score, confusion_matrix, recall_score
from sklearn.metrics import r2_score 
from sklearn.metrics import roc_curve, auc
from sklearn.model_selection import cross_val_score
   
1) Confusion Metrix in KNN, NaiveBayes and LogisticRegression (Only for classification algorithms)
   
   from sklearn.metrics import confusion_matrix
   
   print(confusion_matrix(y_test, y_pred))
   print(accuracy_score(y_test, y_pred))
   print(recall_score(y_test, y_pred))
    
   [[34  2]   //* Here 34 +ve predicted correctly and 2 +ve predicted wrong
    [ 5 54]   //* Here 54 -ve predicted correctly and 5 -ve predicted wrong (Total DF count = 34+2+5+54=95) 

   print(classification_report(y_test, y_pred, target_names=target_names))	  #prints => precision    recall  f1-score   support

2) Score calculation in Linear Regression, Lasso and Ridge 
 
   regression_model.score(X_test, y_test) 
   lasso.score(X_test, y_test) 
   ridge.score(X_test, y_test) 
   
   jointplot can be used to check linearity between actual and predicted  - 
   
   sns.set(style="darkgrid", color_codes=True)
   with sns.axes_style("white"):
		sns.jointplot(x=y_test, y=y_pred, kind="reg", color="k");
   
   Sum Of Squared Errors - 
   ---------------------------
   mse = np.mean((regression_model.predict(X_test)-y_test)**2)
   OR 
   import math
   math.sqrt(mse)

3) Accuracy in NaiveBayes and LogisticRegression
   
   from sklearn.metrics import accuracy_score
   accuracy_score(y_test, prediction) 

4) Evaluating each model using KFold (Ex - DecisionTree_Wine)

    models = []
    models.append(('DecisionTree', Dt_model))
    models.append(('RandomForest', Rf_model))
    # evaluate each model in turn
    results = []
    names = []
    scoring = 'recall'
    for name, model in models:
	
	## If True is set as KFold(5,True) it means shuffling to be done before KFold is done.We cannot have K>number of datapoints
	## If K=number of datapoints then this is called as Leave One Out Cross Validation or LOOCV
    
     	kfold = model_selection.KFold(n_splits=5,random_state=2)    
    	cv_results = model_selection.cross_val_score(model, X, y, cv=kfold, scoring=scoring)
    	results.append(cv_results)
    	names.append(name)
    	msg = "%s: %f (%f)" % (name, np.mean(cv_results), cv_results.var())
    	print(msg)
    # boxplot algorithm comparison

    fig = plt.figure()
    fig.suptitle('Algorithm Comparison')
    ax = fig.add_subplot(111)
    plt.boxplot(results)
    ax.set_xticklabels(names)
    plt.show()
	
5)  ROC, AUC 

    probas1=LRClassifier.fit(X_train,y_train).predict_proba(X_test) 
	probas2=SVCClassifier.fit(X_train,y_train).predict_proba(X_test) 
	
	ROC calculation and AUC  - 
	---------------------------
	fpr1,tpr1, threshold1 = roc_curve(y_test,probas1[:,1])
	roc_auc1 = auc(fpr1,tpr1) 
	
	ROC plot - 
	-----------
    pl.clf()
	pl.plot(fpr1,tpr1,label='ROC curve for logistic (area=%0.2f)' % roc_auc1)
	pl.plot(fpr2,tpr2,label='ROC curve for SVC (area=%0.2f)' % roc_auc2)
	pl.plot([0,1],[0,1],'k--')
	pl.xlim([0.0,1.0])
	pl.ylim([0.0,1.0])
	pl.xlabel('FPR')
	pl.ylabel('TPR')
	pl.title('ROC example')
	pl.legend(loc='lower right')
	pl.show()

############################################################################################################################################################	

																Pipeline 

############################################################################################################################################################	

1)  Pipeline - 

    pipe = Pipeline([("scalar",MinMaxScaler()),("lr",logisticregression())])
	pipe.fit(X_train,y_train)
	pipe.score(X_test,y_test)
    pipe.predict(X_test)
	
	OR 
	
	pipe=make_pipeline(MinMaxScaler(),(SVC()))
	print("Pipeline Steps:\n{}".format(pipe.steps))

############################################################################################################################################################	

																Recommendation Systems

############################################################################################################################################################

Packages - 
-----------
# Association Analysis Recommendation Systems - 
   
from mlxtend.frequent_patterns import apriori
from mlxtend.frequent_patterns import association_rules

# Content Based Recommendation Systems - 

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS
from sklearn.metrics.pairwise import linear_kernel

# Collaborative Filtering Recommendation Systems (Memory Based)  

from surprise import Dataset,Reader
from surprise.model_selection import train_test_split
from surprise import KNNWithMeans
from surprise import accuracy
from surprise import Prediction

# Collaborative Filtering Recommendation Systems (Model Based)
from surprise import SVD
   
1) Association Rules Market Based Analysis Recommendation Systems

    #Sales for France (to keep the data small)
    basket = (df[df['Country'] =="France"]
              .groupby(['InvoiceNo', 'Description'])['Quantity']
              .sum().unstack().reset_index().fillna(0)
              .set_index('InvoiceNo'))

    #any positive values are converted to a 1 and anything less the 0 is set to 0.
    def encode_units(x):
        if x <= 0:
            return 0
        if x >= 1:
            return 1
    basket_sets = basket.applymap(encode_units)

    frequent_itemsets = apriori(basket_sets, min_support=0.07, use_colnames=True)
	rules = association_rules(frequent_itemsets, metric="lift", min_threshold=1)

2) Content Based Recommendation Systems 

    tf = TfidfVectorizer(analyzer='word', ngram_range=(1, 3), min_df=2, stop_words='english')
    tfidf_matrix = tf.fit_transform(df['description'])
    print(tfidf_matrix)
	
	cosine_similarities = linear_kernel(tfidf_matrix, tfidf_matrix)

    titles = df['title']
    titles.shape
    indices = pd.Series(df.index, index=df['title'])
    
	def recommend(title):
        idx = indices[title]
        sim_scores = list(enumerate(cosine_similarities[idx]))
        sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)
        sim_scores = sim_scores[1:31]
        movie_indices = [i[0] for i in sim_scores]
        return titles.iloc[movie_indices]
    	
    recommend('The Godfather').head(10)

3) Collaborative Filtering Based Recommendation Systems 

    algo = KNNWithMeans(k=51, sim_options={'name': 'pearson', 'user_based': False})
    algo.fit(trainset)
    
    # Evalute on test set
    test_pred = algo.test(testset)
    
    # compute RMSE
    accuracy.rmse(test_pred)

    - SVD Based Recommendation

    svd_model = SVD(n_factors=50,biased=False)
    svd_model.fit(trainset)
    
    test_pred = svd_model.test(testset)
	accuracy.rmse(test_pred)

############################################################################################################################################################

              												    Hyperparameter Tuning 

############################################################################################################################################################

from sklearn.model_selection import StratifiedKFold
from sklearn.model_selection import RepeatedStratifiedKFold
from sklearn.model_selection import KFold
from sklearn.model_selection import GridSearchCV,RandomizedSearchCV

1) GridSearchCV 

   # Choose the type of classifier. 
   clf = RandomForestClassifier()
   
   # Choose some parameter combinations to try
   parameters = {'n_estimators': [4, 6, 9],  					#np.arange(4,100)
                 'max_features': ['log2', 'sqrt','auto'], 
                 'criterion': ['entropy', 'gini'],
                 'max_depth': [2, 3, 5, 10], 
                 'min_samples_split': [2, 3, 5],
                 'min_samples_leaf': [1,5,8]
                }
   from sklearn.metrics import make_scorer
   # Type of scoring used to compare parameter combinations
   acc_scorer = make_scorer(accuracy_score)
   
   # Run the grid search
   grid_obj = GridSearchCV(clf, parameters, scoring=acc_scorer)
   grid_obj = grid_obj.fit(X_train, y_train)         => All various combination of models will be built
   
   grid_obj.best_params_                             => Extracts best parameters 
   grid_obj.cv_results_['params']                    => All combinations of parameters 
   grid_obj.cv_results_['mean_test_score']           => Score for all combination of parameters
   grid_obj.best_index                               => What is the index of best hyperparameters combination 
   
   y_pred=grid_obj.predict(X_test)
   
   # Set the clf to the best combination of parameters
   clf = grid_obj.best_estimator_
   
   # Fit the best algorithm to the data. 
   clf.fit(X_train, y_train)

   #CV value in GridSearchCV() determines the cross-validation splitting strategy. Possible inputs for cv are:
    - None, to use the default 5-fold cross validation,
    - Integer, to specify the number of folds in a (Stratified)KFold,

2)  RandomizedSearchCV

    clf = RandomForestClassifier(n_estimators=50)

    param_dist={"max_depth": [3,None],
	            "max_features" : sp_randint(1,11),
				"min_samples_split": sp_randint(2,11),
				"min_samples_leaf" : sp_randint(1,11),
				"bootstrap":[True,False],
				"criterion":[""gini","entropy"]}
				
	#number of random samples 
	samples=10
	randomCV=RandomizedSearchCV(clf,param_distributions=param_dist,n_iter=samples)
	randomCV.fit(X,y)

3)  Ridge - 
    
    alphas = np.array([1,0.1,0.01,0.001,0.0001,0])
    # create and fit a ridge regression model, testing each alpha
    model = Ridge()
    
	grid = GridSearchCV(estimator=model, param_grid=dict(alpha=alphas))
    grid.fit(dataset.data, dataset.target)
    print(grid)
    
	# summarize the results of the grid search
    print(grid.best_score_)
    print(grid.best_estimator_.alpha)
    # prepare a uniform distribution to sample for the alpha parameter
    param_grid = {'alpha': sp_rand()}
    # create and fit a ridge regression model, testing random alpha values
    model = Ridge()
    rsearch = RandomizedSearchCV(estimator=model, param_distributions=param_grid, n_iter=100)
    rsearch.fit(dataset.data, dataset.target)
    print(rsearch)
    # summarize the results of the random parameter search
    print(rsearch.best_score_)
    print(rsearch.best_estimator_.alpha)

	OR 
	
	model = RidgeClassifier()
    alpha = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]
    
	# define grid search
    grid = dict(alpha=alpha)
    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)
    grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)
    grid_result = grid_search.fit(X, y)
    
	# summarize results
    print("Best: %f using %s" % (grid_result.best_score_, grid_result.best_params_))
    means = grid_result.cv_results_['mean_test_score']
    stds = grid_result.cv_results_['std_test_score']
    params = grid_result.cv_results_['params']
    for mean, stdev, param in zip(means, stds, params):
        print("%f (%f) with: %r" % (mean, stdev, param))

4) XGClassifier - 
    
    # grid search
    model = XGBClassifier()
    n_estimators = [50, 100, 150, 200]
    max_depth = [2, 4, 6, 8]
    param_grid1 = dict(max_depth=max_depth, n_estimators=n_estimators)
    
    kfold1 = StratifiedKFold(n_splits=10, shuffle=True, random_state=7)
    grid_search1 = GridSearchCV(model1, param_grid1, scoring="accuracy", n_jobs=-1, cv=kfold1, verbose=1)
    grid_result1 = grid_search1.fit(Xtrain, ytrain)

	# summarize results
    print("Best: %f using %s" % (grid_result1.best_score_, grid_result1.best_params_))
    means = grid_result1.cv_results_['mean_test_score']
    stds = grid_result1.cv_results_['std_test_score']
    params = grid_result1.cv_results_['params']
    for mean, stdev, param in zip(means, stds, params):
    	print("%f (%f) with: %r" % (mean, stdev, param))
    
	# plot results
    scores = numpy.array(means).reshape(len(max_depth), len(n_estimators))
    for i, value in enumerate(max_depth):
        pyplot.plot(n_estimators, scores[i], label='depth: ' + str(value))
    pyplot.legend()
    pyplot.xlabel('n_estimators')
    pyplot.ylabel('Log Loss')
    pyplot.savefig('n_estimators_vs_max_depth.png')

5)  LogisticRegression 

	solvers = ['newton-cg', 'lbfgs', 'liblinear']
    penalty = ['l2']
    c_values = [100, 10, 1.0, 0.1, 0.01]
    
	# define grid search
    grid = dict(solver=solvers,penalty=penalty,C=c_values)
    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)
    grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)
    grid_result = grid_search.fit(X, y)
    
	# summarize results
    print("Best: %f using %s" % (grid_result.best_score_, grid_result.best_params_))
    means = grid_result.cv_results_['mean_test_score']
    stds = grid_result.cv_results_['std_test_score']
    params = grid_result.cv_results_['params']
    for mean, stdev, param in zip(means, stds, params):
        print("%f (%f) with: %r" % (mean, stdev, param))

6)  KNN 

    model = KNeighborsClassifier()
    n_neighbors = range(1, 21, 2)
    weights = ['uniform', 'distance']
    metric = ['euclidean', 'manhattan', 'minkowski']
    
	# define grid search
    grid = dict(n_neighbors=n_neighbors,weights=weights,metric=metric)
    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)
    grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)
    grid_result = grid_search.fit(X, y)
    
	# summarize results
    print("Best: %f using %s" % (grid_result.best_score_, grid_result.best_params_))
    means = grid_result.cv_results_['mean_test_score']
    stds = grid_result.cv_results_['std_test_score']
    params = grid_result.cv_results_['params']
    for mean, stdev, param in zip(means, stds, params):
        print("%f (%f) with: %r" % (mean, stdev, param))

7)  SVM 

    model = SVC()
    kernel = ['poly', 'rbf', 'sigmoid']
    C = [50, 10, 1.0, 0.1, 0.01]
    gamma = ['scale']
    
	# define grid search
    grid = dict(kernel=kernel,C=C,gamma=gamma)
    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)
    grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)
    grid_result = grid_search.fit(X, y)
    
	# summarize results
	print("Best: %f using %s" % (grid_result.best_score_, grid_result.best_params_))
    means = grid_result.cv_results_['mean_test_score']
    stds = grid_result.cv_results_['std_test_score']
    params = grid_result.cv_results_['params']
    for mean, stdev, param in zip(means, stds, params):
        print("%f (%f) with: %r" % (mean, stdev, param))

8)  BaggingClassifier - 

    # define models and parameters
    model = BaggingClassifier()
    n_estimators = [10, 100, 1000]
    
	# define grid search
    grid = dict(n_estimators=n_estimators)
    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)
    grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)
    grid_result = grid_search.fit(X, y)
    
	# summarize results
    print("Best: %f using %s" % (grid_result.best_score_, grid_result.best_params_))
    means = grid_result.cv_results_['mean_test_score']
    stds = grid_result.cv_results_['std_test_score']
    params = grid_result.cv_results_['params']
    for mean, stdev, param in zip(means, stds, params):
        print("%f (%f) with: %r" % (mean, stdev, param))

9) RandomForestClassifier - 

   # define models and parameters
   model = RandomForestClassifier()
   n_estimators = [10, 100, 1000]
   max_features = ['sqrt', 'log2']
   
   # define grid search
   grid = dict(n_estimators=n_estimators,max_features=max_features)
   cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)
   grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)
   grid_result = grid_search.fit(X, y)
   
   # summarize results
   print("Best: %f using %s" % (grid_result.best_score_, grid_result.best_params_))
   means = grid_result.cv_results_['mean_test_score']
   stds = grid_result.cv_results_['std_test_score']
   params = grid_result.cv_results_['params']
   for mean, stdev, param in zip(means, stds, params):
       print("%f (%f) with: %r" % (mean, stdev, param))

10) GradientBoostingClassifier - 

   # define models and parameters
   model = GradientBoostingClassifier()
   n_estimators = [10, 100, 1000]
   learning_rate = [0.001, 0.01, 0.1]
   subsample = [0.5, 0.7, 1.0]
   max_depth = [3, 7, 9]
   
   # define grid search
   grid = dict(learning_rate=learning_rate, n_estimators=n_estimators, subsample=subsample, max_depth=max_depth)
   cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)
   grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)
   grid_result = grid_search.fit(X, y)
   
   # summarize results
   print("Best: %f using %s" % (grid_result.best_score_, grid_result.best_params_))
   means = grid_result.cv_results_['mean_test_score']
   stds = grid_result.cv_results_['std_test_score']
   params = grid_result.cv_results_['params']
   for mean, stdev, param in zip(means, stds, params):
       print("%f (%f) with: %r" % (mean, stdev, param))

#################################################################################################################################################

															AUTOML

#################################################################################################################################################

autosklearn => For ML 
autokeras   => For DeepLearning 
Tpot 

Autosklearn - 
---------------

This works only in Google Collab. 

!apt-get install swig -y
!pip install Cython numpy 
!pip install auto-sklearn 

import autosklearn.classification as classifier 

automlclassifier = classifier.AutosklearnClassifier(time_left_for_this_task=180,per_run_time_limit=40)
automlclassifier.fit(X_train,y_train)
y_pred=automlclassifier.predict(X_test)
score=accuracy_score(y_test,y_pred)
print(score)
automlclassifier.show_models()

TPot - 
-------
conda install -c conda-forge tpot

from tpot import TPOTClassifier

tpot = TPOTClassifier(verbosity=2, max_time_mins=10)
tpot.fit(X_train, y_train)
print(tpot.score(X_test, y_test))
tpot.fitted_pipeline_
print(tpot.score(X_test, y_test))
tpot.export('tpot_iris_pipeline.py')


############################################################################################################################################################

																		Statistics 

############################################################################################################################################################

CDF,ISF
--------

P(value<sample_mean) = stats.norm.cdf(sample_mean,loc=pop_mean,scale=pop_variance)
P(value>sample_mean) = 1-stats.norm.cdf(sample_mean,loc=pop_mean,scale=pop_variance)
P(90<value<110)      = stats.norm.cdf(110,loc=100,scale=10) - stats.norm.cdf(90,loc=100,scale=10)

For 0.01 probability, z=stats.norm.isf(0.01,loc=100,scale=10)

By default, norm.cdf takes standard normal distribution, for known Z value, 
P(value falling less than Z) = stats.norm.cdf(Z)

Z-test 
--------
Z=(Xbar - mu)/SE
Standard error(SE) = SD of population/sqrt(N)

Critical Values for two-tailed    => stats.norm.isf(0.025,loc=mu,scale=se) and stats.norm.isf(0.975,loc=mu,scale=se)
Critical Z-value for right tailed => stats.t.isf(0.05,df=n-1,loc=Population mean,scale=Standard error)
Critical Z-value for left tailed  => stats.t.isf(0.95,df=n-1,loc=Population mean,scale=Standard error)

t-test 
--------
tstat= (xbar - mu) / se

Paired t-test 
--------------
t_statistic,p_value = ttest_1samp(post-pre,0)
print(t_statistic,p_value)

If not normal -> 
z_statistic,p_value = wilcoxon(post-pre)

Check for normality -> 
shapiro(post-pre) If p>0.05 (means normal)

Check for equal variance -> 
levene(a, b) If p>0.05 (equal variance)  

Unpaired t-test 
----------------
t_statistic, p_value = ttest_ind(group1, group2)
print(t_statistic,p_value)

mannwhitneyu test for non parametric test 

Chi Square test 
----------------
Similar to corr in continuous variables but its for categorical variables 

1) Univariate 
   H0: All proportions are same, Ha: Atlease one has different proportions
   chisquare(DF['cat_col'].value_counts())  #If p-value > 0.05 all proportions are same 
   
2) Bivariate 
   H0: Two categorical variables are independant , Ha: Two categorical variables are dependant
   cont = pd.crostab(DF["cat_col1"],DF["cat_col2"])
   chi2_contingency(cont)

ANNOVA test 
------------
F, p = f_oneway(
    dst.loc[dst.Occupation == "SAL", "bal_cap"],
    dst.loc[dst.Occupation == "SELF-EMP", "bal_cap"],
    dst.loc[dst.Occupation == "PROF", "bal_cap"],
    dst.loc[dst.Occupation == "SENP", "bal_cap"]
    )

mod = ols('Monthly_inc ~ Gym', data = monthly_inc_df).fit()
aov_table = sm.stats.anova_lm(mod, typ=2)  # The type of Anova test to perform here is 2

mc = MultiComparison(dst['bal_cap'], dst['Occupation'])
result = mc.tukeyhsd()

Bartlett test of homogenity
------------------------------
## Bartlett test of homogenity of variances
## Null Hypothesis : Homogeneous
## Alternate Hypothesis : Not Homogeneous

Kurskal-Wallis test
--------------------
stat, p = kruskal(
    dst.loc[dst.Occupation == "SAL", "bal_cap"],    dst.loc[dst.Occupation == "SELF-EMP", "bal_cap"],
    dst.loc[dst.Occupation == "PROF", "bal_cap"],    dst.loc[dst.Occupation == "SENP", "bal_cap"]
    )
alpha = 0.05
if p > alpha:
    print('Same distributions (fail to reject H0)')
else:
    print('Different distributions (reject H0)')
	
Normal Distributions Test 
----------------------------

1) Histogram (DS=pd.Series([1,2,3,4,5,6,7,8,9,10]) DS.hist())
2) Box Plot  (DS=pd.Series([1,2,3,4,5,6,7,8,9,10]) DS.plot(kind='box')) => Good to analyse multiple variables distribution
3) QQ Plot   (data=norm.rvs(size=1000) sm.qqplot(data,line='45')  pylab.show()
4) Kolmogorov Smirnov test (if p-value < 0.05 => Normal Distribution) (data=norm.rvs(size=1000) ks_statistic, p_value = kstest(data,'norm') => Not much power (Requires large # of observations and sensitivr to reject outliers) 
5) Lilliefors test (if p-value < 0.05 => Normal Distribution) (data=norm.rvs(size=500) lilliefors(data) => Power is still lower than the Shapiro Wilk test 
6) Shapiro Wilk test (if p-value < 0.05 => Normal Distribution) (data=norm.rvs(size=500) shapiro(data)) => Most powerful test  

############################################################################################################################################################

				 											Python Learnings 

############################################################################################################################################################

Lambda Function - 
-------------------
f = lambda x, y: x + y
f(1, 2)

Map Function - 
---------------
seq=[1,2,3,4,5]
list(map(times2,seq)) => [2,4,6,8,10]

def times2(a):
    return a*2
	
OR 

list(map(lambda a:a*2,seq)) => [2,4,6,8,10]

Filter Function - 
------------------
Returns value which is true 
seq=[1,2,3,4,5]
filter(lambda a:a%2 ==0, seq) => [2,4]

Method - 
-----------
S="Hi #hello #how are you"

S.split()       => ['Hi','hello','how','are','you'] #Split based on space 
S.split('#')    => ['Hi','hello','how are you']     #Split based on #
S.split('#')[1] => 'hello'
S.lower(), S.upper()

Pop - 
--------
lst=[1,2,3,4,5]
item=lst.pop()
first=lst.pop(0)
lst            => [2,3,4]
item           => 5
first          => 1

'hi' in S.lower().split() => True 

Arrays vs Lists - 
------------------
We cant perform math operation on lists, but we can do it on arrays.
Arrays needs to be declared.

Arrays - 
----------
from numpy.random import randint => we can use directly randint(5) instead of np.random.randint(5)

np.array(list)
np.arange(0,11,2)
np.zeros((5,5))
np.ones((3,4))
np.linspace(0,5,10)(start,end,number of elements   => 0 to 5 array with 10 evenly spaced elements 
np.eye(4)                              => Identity Matrix of 4by4 (if 2 square brackets[[ means 2-D matrix)
np.random.seed(101)
np.random.rand(5)                      => 1-D Random samples of uniform distribution from 0 to 1 (for 2-D pass tuples) 
np.random.randn(2)                     => Standard normal distribution 
np.random.randint(low,high,actualsize) Ex- arr=np.random.randint(0,50,10) => array([35,18,26,49,34,48,3,30,6,6])
arr.reshape(2,5)                       => Reshape array to 2-D
arr.min(),arr.max()
arr.argmax()                           => Index location of max value 
arr.shape
arr.dtype
np.arange(9).reshape(3,3)              => 3*3 Matrix from 0 to 8 
arr_matrix.sum(axis=0)                 => Sum of all columns in the matrix 

Copy in array - 
------------------
arr = array([35,18,26,49,34,48,3,30,6,6])
slice_arr = arr[0:5]                  => array([35,18,26,49,34])
slice_arr[:] = 99                     => slice_arr = array([99,99,99,99,99])
it changes original array also (Broadcasting) => arr = array([99,99,99,99,99,48,3,30,6,6])

arr_copy = arr.copy()                 => Change in arr_copy is not affected in original array (i.e.no effect of broadcasting)

Array Indexing - 
-----------------
arr[1,2] is same as arr[1][2]

Boolean Array - 
-----------------
arr = array([35,18,26,49,34,48,3,30,6,6])
arr[arr<10] 						 => array([3,6,6])

Array Operations - 
-------------------
arr + arr (+,-,*), arr * 2 

Series - 
----------
Index with names but no column names 
ser = pd.Series(data,labels) 

column in pandas are of type series 

Pandas - 
---------
For fetching multiple columns DF[['a','b']] (pass list of column names) 

loc => Row Label based 
For fetching multiple rows DF.loc[['a','b']]
For fetching particular cell DF.loc['row','col']
iloc => Row index based 

DF[DF<0] returns dataframe with null values at positions where value is less than 0 
DF[DF['a']>0] returns rows for which column a values are greater than 0 
DF[(DF['a']>0) and (DF['b']>0)] => This will give error so use '&' '|' instead of 'and' 'or' since 'and' on series will not work 

DF.reset_index(inplace=True) resets column index 0,1,2,...
DF.set_index('col_name')
DF.index.names=name   # Setting name to index column 

Multilevel index => DF.xs('G1')
dropna has threshold

Crosstab - 
-----------
>>> a = np.array(["foo",   "foo",   "foo",   "foo",   "bar",   "bar",   "bar",   "bar",   "foo",   "foo",   "foo"], dtype=object)
>>> b = np.array(["one",   "one",   "one",   "two",   "one",   "one",   "one",   "two",   "two",   "two",   "one"], dtype=object)
>>> c = np.array(["dull",  "dull",  "shiny", "dull",  "dull",  "shiny", "shiny", "dull",  "shiny", "shiny", "shiny"],dtype=object)
>>> pd.crosstab(a, [b, c], rownames=['a'], colnames=['b', 'c'])

b   one        two
c   dull shiny dull shiny
a
bar    1     2    1     0
foo    2     2    1     2

Covariance Matrix - 
---------------------
cov_matrix = np.cov(X_std.T)
print('Covariance Matrix \n%s', cov_matrix)

Remove those features that have strong correlation with each other (Multicolinearity) 

############################################################################################################################################################